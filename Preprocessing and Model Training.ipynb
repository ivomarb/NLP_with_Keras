{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate: Ivomar Brito Soares\n",
    "\n",
    "Email: ivomarbsoares@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Importing libraries</li>\n",
    "    <li>Utility methods</li>\n",
    "    <li>Reading data set</li>\n",
    "    <li>Preprocessing</li>\n",
    "    <li>Feature Extraction: Term Frequency - Inverse Document Frequency (TF-IDF)</li>\n",
    "    <li>Preparing categorical target variable</li>\n",
    "    <li>Training deep learning model</li>\n",
    "    <li>Model Evaluation</li>\n",
    "    <li>Saving model to file</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ivomar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ivomar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Data pre-processing modules\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# TFIDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Deep Learning modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(dataset, feature_name):\n",
    "    \"\"\"\n",
    "    These are the basic pre-processing steps followed in this function:\n",
    "    - Convert text to lower case.\n",
    "    - Punctuation removal.\n",
    "    - Stop words removal.\n",
    "    \n",
    "    Additional possible pre-processing steps (future work):\n",
    "    - Common words removal.\n",
    "    - Rare words removal.\n",
    "    - Spelling correction.\n",
    "    - Keeping words of length of at least 3.\n",
    "    \"\"\"   \n",
    "    # The first pre-processing is to convert all text into lower case, this avoids having multiple copies\n",
    "    # of the same words.\n",
    "    dataset[feature_name] = dataset[feature_name].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    \n",
    "    # Punctuation removal, often it does not add extra information when dealing with text data. Removing them helps\n",
    "    # reduce the size of the training data.\n",
    "    dataset[feature_name] = dataset[feature_name].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    # Stop words (frequently occurring words) should be removed from the dataset.\n",
    "    stop = stopwords.words('english')\n",
    "    dataset[feature_name] = dataset[feature_name].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    \n",
    "    # Lemmatization: Converts the word into its root word.\n",
    "    dataset[feature_name] = dataset[feature_name].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    \n",
    "\n",
    "def prepare_targets(y_train):\n",
    "    \"\"\"\n",
    "    Converts non-numerical catorigal labels to numerical categorical labels.\n",
    "    \"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    return y_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         today past read title wine rich full aged lee ...\n",
       "1         crisp dry searing acidity 100 varietal wine co...\n",
       "2         light lovely 2 residual sugar taste drier arre...\n",
       "3         borras blend 80 petite sirah 10 syrah 10 mourv...\n",
       "4         spirit south africa swartland region shine rus...\n",
       "                                ...                        \n",
       "103971    textured full wine ripe character full fragran...\n",
       "103972    funk nose soon blow reveal generously ripe fru...\n",
       "103973    flinty lemon caramel flouted around rich layer...\n",
       "103974    exuberantly fragrant ripe tropical fruit flora...\n",
       "103975    wine brings fruitiness gamay along extra perfu...\n",
       "Name: features, Length: 103927, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping missing values\n",
    "dataset.dropna(subset=['categorical_target_1'], inplace=True)\n",
    "\n",
    "basic_preprocessing(dataset, 'features')\n",
    "dataset['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence. TF = (Number of times term T appears in the particular row) / (number of terms in that row).</li>\n",
    "    <li>The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if itâ€™s appearing in all the documents. Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.</li>\n",
    "    <li>TF-IDF is the multiplication of the TF and IDF which is shown above.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english', max_features= 10000,strip_accents='unicode', norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer.fit_transform(dataset['features']).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing categorical target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = prepare_targets(dataset['categorical_target_1'])\n",
    "\n",
    "nb_classes = 43      # Chosen target variable, categorical_target_1 with 43 unique values or classes.\n",
    "\n",
    "# Converts the 43 categories into one-hot encoding vectors in which 43 columns\n",
    "# are created and the values against the respective classes are given as 1. All other classes are given as 0.\n",
    "y_train = np_utils.to_categorical(y_train_enc, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103927, 6)\n",
      "(103927, 10000)\n",
      "(103927, 43)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "batch_size = 64\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model built in keras\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000,input_shape= (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              10001000  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                25050     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 43)                2193      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 43)                0         \n",
      "=================================================================\n",
      "Total params: 10,528,743\n",
      "Trainable params: 10,528,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "103927/103927 [==============================] - 319s 3ms/step - loss: 0.8662\n",
      "Epoch 2/20\n",
      "103927/103927 [==============================] - 311s 3ms/step - loss: 0.4840\n",
      "Epoch 3/20\n",
      "103927/103927 [==============================] - 283s 3ms/step - loss: 0.3503\n",
      "Epoch 4/20\n",
      "103927/103927 [==============================] - 386s 4ms/step - loss: 0.2571\n",
      "Epoch 5/20\n",
      "103927/103927 [==============================] - 394s 4ms/step - loss: 0.1862\n",
      "Epoch 6/20\n",
      "103927/103927 [==============================] - 394s 4ms/step - loss: 0.1372\n",
      "Epoch 7/20\n",
      "103927/103927 [==============================] - 395s 4ms/step - loss: 0.1107\n",
      "Epoch 8/20\n",
      "103927/103927 [==============================] - 394s 4ms/step - loss: 0.0868\n",
      "Epoch 9/20\n",
      "103927/103927 [==============================] - 395s 4ms/step - loss: 0.0767\n",
      "Epoch 10/20\n",
      "103927/103927 [==============================] - 394s 4ms/step - loss: 0.0651\n",
      "Epoch 11/20\n",
      "103927/103927 [==============================] - 368s 4ms/step - loss: 0.0563\n",
      "Epoch 12/20\n",
      "103927/103927 [==============================] - 305s 3ms/step - loss: 0.0528\n",
      "Epoch 13/20\n",
      "103927/103927 [==============================] - 305s 3ms/step - loss: 0.0495\n",
      "Epoch 14/20\n",
      "103927/103927 [==============================] - 308s 3ms/step - loss: 0.0453\n",
      "Epoch 15/20\n",
      "103927/103927 [==============================] - 306s 3ms/step - loss: 0.0411\n",
      "Epoch 16/20\n",
      "103927/103927 [==============================] - 305s 3ms/step - loss: 0.0396\n",
      "Epoch 17/20\n",
      "103927/103927 [==============================] - 308s 3ms/step - loss: 0.0374\n",
      "Epoch 18/20\n",
      "103927/103927 [==============================] - 310s 3ms/step - loss: 0.0342\n",
      "Epoch 19/20\n",
      "103927/103927 [==============================] - 309s 3ms/step - loss: 0.0330\n",
      "Epoch 20/20\n",
      "103927/103927 [==============================] - 310s 3ms/step - loss: 0.0323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f2cb09f4c50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predclass = model.predict_classes(X_train,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural Network - Train accuracy:\n",
      "Deep Neural Network - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3016\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      1.00      1.00      1845\n",
      "           3       1.00      1.00      1.00      2703\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.81      0.42      0.56        40\n",
      "           6       0.99      0.99      0.99       114\n",
      "           7       1.00      0.99      1.00       199\n",
      "           8       1.00      1.00      1.00      3587\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.74      0.80      0.77        56\n",
      "          11       1.00      0.50      0.67         8\n",
      "          12       0.00      0.00      0.00        10\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.98      1.00      0.99        59\n",
      "          15       1.00      1.00      1.00     17713\n",
      "          16       1.00      0.96      0.98        72\n",
      "          17       1.00      1.00      1.00      1739\n",
      "          18       0.99      1.00      1.00       380\n",
      "          19       0.96      1.00      0.98       120\n",
      "          20       0.00      0.00      0.00         7\n",
      "          21       0.99      1.00      1.00       411\n",
      "          22       1.00      1.00      1.00     15660\n",
      "          23       0.88      0.28      0.42        25\n",
      "          24       0.00      0.00      0.00         5\n",
      "          25       0.00      0.00      0.00        10\n",
      "          26       0.81      0.82      0.81        56\n",
      "          27       0.48      1.00      0.65        45\n",
      "          28       0.89      0.36      0.52        22\n",
      "          29       1.00      1.00      1.00      1128\n",
      "          30       0.00      0.00      0.00        14\n",
      "          31       1.00      1.00      1.00      4530\n",
      "          32       0.92      0.98      0.95        89\n",
      "          33       0.80      0.44      0.57         9\n",
      "          34       0.00      0.00      0.00         1\n",
      "          35       0.98      0.97      0.98        66\n",
      "          36       1.00      1.00      1.00      1101\n",
      "          37       1.00      1.00      1.00      5346\n",
      "          38       0.00      0.00      0.00         4\n",
      "          39       0.89      0.99      0.93        71\n",
      "          40       1.00      1.00      1.00     43568\n",
      "          41       0.50      0.17      0.25        12\n",
      "          42       0.70      0.93      0.80        81\n",
      "\n",
      "    accuracy                           1.00    103927\n",
      "   macro avg       0.68      0.64      0.65    103927\n",
      "weighted avg       1.00      1.00      1.00    103927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Deep Neural Network - Train Classification Report\")\n",
    "print (classification_report(y_train_enc,y_train_predclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Serialize model to JSON.\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# Serialize weights to HDF5.\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
