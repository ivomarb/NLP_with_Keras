{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate: Ivomar Brito Soares\n",
    "\n",
    "Email: ivomarbsoares@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul></ul>\n",
    "<li>Importing libraries</li>\n",
    "<li>Utility methods</li>\n",
    "<li>Defining variables</li>\n",
    "<li>Reading data set</li>\n",
    "<li>Preprocessing</li>\n",
    "<li>Feature Extraction: Term Frequency - Inverse Document Frequency (TF-IDF)</li>\n",
    "<li>Preparing categorical target variable</li>\n",
    "<li>Loading trained deep learning model</li>\n",
    "<li>Model evaluation and performance report</li>\n",
    "<ul></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Data pre-processing modules\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# TFIDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(dataset, feature_name):\n",
    "    \"\"\"\n",
    "    These are the basic pre-processing steps followed in this function:\n",
    "    - Convert text to lower case.\n",
    "    - Punctuation removal.\n",
    "    - Stop words removal.\n",
    "    \n",
    "    Additional possible pre-processing steps (future work):\n",
    "    - Common words removal.\n",
    "    - Rare words removal.\n",
    "    - Spelling correction.\n",
    "    - Keeping words of length of at least 3.\n",
    "    \"\"\"   \n",
    "    # The first pre-processing is to convert all text into lower case, this avoids having multiple copies\n",
    "    # of the same words.\n",
    "    dataset[feature_name] = dataset[feature_name].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    \n",
    "    # Punctuation removal, often it does not add extra information when dealing with text data. Removing them helps\n",
    "    # reduce the size of the training data.\n",
    "    dataset[feature_name] = dataset[feature_name].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    # Stop words (frequently occurring words) should be removed from the dataset.\n",
    "    stop = stopwords.words('english')\n",
    "    dataset[feature_name] = dataset[feature_name].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    \n",
    "    # Lemmatization: Converts the word into its root word.\n",
    "    dataset[feature_name] = dataset[feature_name].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    \n",
    "def prepare_targets(y_train):\n",
    "    \"\"\"\n",
    "    Converts non-numerical catorigal labels to numerical categorical labels.\n",
    "    \"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    return y_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change these variables to the desired valuables to load and evaluate the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_evaluation_dataset = 'test_data.csv'\n",
    "categorical_target_name = 'categorical_target_1'\n",
    "features_column_name = 'features'\n",
    "nb_classes = 43      # Chosen target variable, categorical_target_1 with 43 unique values or classes.\n",
    "model_json_file = 'model.json'\n",
    "model_h5_file = 'model.h5'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(path_to_evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missing values\n",
    "dataset.dropna(subset=[categorical_target_name], inplace=True)\n",
    "\n",
    "basic_preprocessing(dataset, features_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english', max_features= 10000,strip_accents='unicode', norm='l2')\n",
    "X_test = tfidf_vectorizer.fit_transform(dataset[features_column_name]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing categorical target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_enc = prepare_targets(dataset[categorical_target_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading trained deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(model_json_file, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(model_h5_file)\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and performance report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predclass = loaded_model.predict_classes(X_test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Deep Neural Network - Test Classification Report\")\n",
    "print (classification_report(y_test_enc,y_test_predclass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
